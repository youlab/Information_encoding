{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,roc_curve,auc\n",
    "from sklearn.utils import shuffle \n",
    "from sklearn.metrics import log_loss,mean_squared_error\n",
    "\n",
    "from resources.data_utils import split_dataset, DataGenerator\n",
    "from resources.model_utils import build_model, train_model_noitr,evaluate_model\n",
    "from resources.savers import TrainingSaver\n",
    "from resources.utils import label_str_to_dec,prediction_standardized,aggregate_generator_labels,label_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STACKED MODEL\n",
    "def stacked_dataset(members, X_generator):        \n",
    "    stackX = None \n",
    "    for model in members:\n",
    "        #yhat = prediction_standardized(evaluate_model(model, evaluation_generator=X_generator, workers=6)) #one-hot\n",
    "        yhat = evaluate_model(model, evaluation_generator=X_generator, workers=6)\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = np.dstack((stackX, yhat))\n",
    "            \n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]),order='F')\n",
    "    return stackX\n",
    "\n",
    "def build_stacked_model(num_layer,num_models,num_classes,optimizer):\n",
    "    model = Sequential()\n",
    "    if num_layer == 0:\n",
    "        model.add(Dense(num_classes,input_dim=num_models*num_classes,activation='softmax')) \n",
    "        \n",
    "    elif num_layer == 1:\n",
    "        model.add(Dense(40, input_dim=num_models*num_classes, activation='relu'))\n",
    "        model.add(Dense(num_classes,input_dim=60,activation='softmax')) \n",
    "        \n",
    "    elif num_layer == 2:\n",
    "        model.add(Dense(60, input_dim=num_models*num_classes, activation='relu'))\n",
    "        model.add(Dense(30, activation='relu'))   \n",
    "        model.add(Dense(num_classes,input_dim=30,activation='softmax'))\n",
    "        \n",
    "    elif num_layer == 3:\n",
    "        model.add(Dense(60, input_dim=num_models*num_classes, activation='relu'))\n",
    "        model.add(Dense(45, activation='relu'))\n",
    "        model.add(Dense(25, activation='relu'))\n",
    "        model.add(Dense(num_classes,input_dim=25,activation='softmax'))\n",
    "        \n",
    "    model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fit stacked model based on the outputs from the ensemble members\n",
    "def fit_stacked_model(model_stack,X_train_stacked,y_train,X_val_stacked,y_val,epochs,batch_size,early_stopping):\n",
    "    history = model_stack.fit(X_train_stacked, \\\n",
    "                              y_train, \\\n",
    "                              validation_data=(X_val_stacked, y_val),\\\n",
    "                              epochs=epochs, \\\n",
    "                              batch_size=batch_size,\\\n",
    "                              callbacks=[early_stopping])\n",
    "    return model_stack,history\n",
    "\n",
    "def plot_EM_trian_history(history,path_out,IFPLOT=True):\n",
    "    fig, axs = plt.subplots(1, 2,figsize=(10, 5))\n",
    "    axs[0].plot(history.history['accuracy'])\n",
    "    axs[0].plot(history.history['val_accuracy'])\n",
    "    axs[0].set_title('Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['Train', 'Test'], loc='upper left')\n",
    "    axs[1].plot(history.history['loss'])\n",
    "    axs[1].plot(history.history['val_loss'])\n",
    "    axs[1].set_title('Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['Train', 'Test'], loc='upper left')\n",
    "    if IFPLOT:\n",
    "        history_figname = path_out+'stacked_model_training_hisotry.png'\n",
    "        plt.savefig(history_figname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR ROC\n",
    "def compute_roc(char_list,y_test,pred_prob):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(len(char_list)):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:,i], pred_prob[:,i]) \n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    return fpr,tpr,roc_auc\n",
    "\n",
    "\n",
    "def plot_roc(axs,label_list,fpr,tpr,roc_auc,title,addlegend=False):\n",
    "    for i in range(len(char_list)):\n",
    "        axs.plot(fpr[i], tpr[i], label='Class {0}'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "    axs.plot([0, 1], [0, 1], 'k--')\n",
    "    axs.set_xlim([0.0, 1.0])\n",
    "    axs.set_ylim([0.0, 1.05])\n",
    "    axs.set_xlabel('False Positive Rate',fontsize=20,fontname='Calibri', color=\"black\")\n",
    "    axs.set_ylabel('True Positive Rate',fontsize=20,fontname='Calibri', color=\"black\")\n",
    "    axs.tick_params(axis='x', labelsize=20,color=\"black\",labelcolor=\"black\") \n",
    "    axs.tick_params(axis='y', labelsize=20,color=\"black\",labelcolor=\"black\")\n",
    "    axs.set_title(title)\n",
    "    if addlegend == True:\n",
    "        axs.legend(loc=\"right\",bbox_to_anchor=(1.3, 0.5))\n",
    "        \n",
    "def ave_roc(label_list,fpr,tpr,roc_auc):\n",
    "    tpr_all = []\n",
    "    roc_auc_all = []\n",
    "    for i in range(len(label_list)):\n",
    "        fpr_current = fpr[i]\n",
    "        tpr_current = tpr[i]\n",
    "        fpr_vals = np.linspace(0, 1, 201)\n",
    "        tpr_current_interp = np.interp(fpr_vals, fpr_current, tpr_current)\n",
    "        tpr_all.append(tpr_current_interp)\n",
    "        roc_auc_current = roc_auc[i]\n",
    "        roc_auc_all.append(roc_auc_current)\n",
    "        \n",
    "    ave_tpr = np.average(tpr_all, axis=0)\n",
    "    std_tpr = np.std(tpr_all, axis=0)\n",
    "    ave_roc_auc = np.average(roc_auc_all)\n",
    "    return ave_tpr,std_tpr,ave_roc_auc\n",
    "\n",
    "def plot_ave_roc(axs,ave_tpr,std_tpr,ave_roc_auc,title,addlegend=False):\n",
    "    \n",
    "    fpr = np.linspace(0, 1, 201)\n",
    "    tprs_upper = np.minimum(ave_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(ave_tpr - std_tpr, 0)\n",
    "    axs.plot(fpr, ave_tpr,linewidth=4)\n",
    "    axs.fill_between(fpr, tprs_lower, tprs_upper, color='grey', alpha=.2)\n",
    "    axs.set_xlim([0.0, 1.0])\n",
    "    axs.set_ylim([0.0, 1.05])\n",
    "    axs.set_xlabel('False Positive Rate',fontsize=20,fontname='Calibri', color=\"black\")\n",
    "    axs.set_ylabel('True Positive Rate',fontsize=20,fontname='Calibri', color=\"black\")\n",
    "    axs.tick_params(axis='x', labelsize=20,color=\"black\",labelcolor=\"black\") \n",
    "    axs.tick_params(axis='y', labelsize=20,color=\"black\",labelcolor=\"black\") \n",
    "    axs.set_title(title,fontsize=20,fontname='Calibri', color=\"black\")\n",
    "    if addlegend == True:\n",
    "        axs.legend(loc=\"right\",bbox_to_anchor=(2, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset and base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_train_size = 1500\n",
    "split_dict = {12000:1,6000:2,3000:3,1500:4,750:5,375:6,150:7}\n",
    "split_idx  = split_dict.get(individual_train_size)\n",
    "\n",
    "dataset = 'dataset_name'\n",
    "path = \"/home/datasets/\"+dataset+'/'\n",
    "path_final = path + \"preprocessed_80/\"\n",
    "path_bm = \"/home/base_models/'\n",
    "path_plaintext = path\n",
    "filename = path_plaintext + 'labels_spot_binary.csv'\n",
    "\n",
    "num_classes = 15 # to change according to the dictionary size\n",
    "char_list = [str(i) for i in range(1, num_classes+1)]\n",
    "char2int = {char: i for i, char in enumerate(char_list)}\n",
    "num_replicates = 1000\n",
    "dataset_size = num_classes * num_replicates\n",
    "image_dimension = (80, 80) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_base_model = 5 # number of base models\n",
    "num_layer = 0 # depth of stacked model\n",
    "\n",
    "# create output folder\n",
    "path_out = path_bm + str(individual_train_size)+\"_HL_\"+str(num_layer)+\"_BM_\"+str(num_base_model)+\"/\"\n",
    "if not os.path.exists(path_out):\n",
    "    os.makedirs(path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename models and txt files\n",
    "for i in range(1,num_base_model+1):\n",
    "    \n",
    "    filename1 = path_bm+'class_15_trainsize_'+str(round(individual_train_size*(1-PORTION)))+'_opt_Adam_batch_size_4_test_'+str(i)+'_accuracy.txt'\n",
    "    if os.path.exists(filename1):\n",
    "        print('RENAMING')\n",
    "        new_filename1 = path_bm+'model_'+str(i)+'_accuracy.txt'\n",
    "        os.rename(filename1,new_filename1)\n",
    "    else: print('NO NEED TO RENAME')\n",
    "        \n",
    "    filename2 = path_bm+'class_15_trainsize_'+str(round(individual_train_size*(1-PORTION)))+'_opt_Adam_batch_size_4_test_'+str(i)+'_model.h5'\n",
    "    if os.path.exists(filename2):\n",
    "        new_filename2 = path_bm+'model_'+str(i)+'.h5'\n",
    "        os.rename(filename2,new_filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "labels = []\n",
    "with open(filename, \"r\") as csvFile:\n",
    "    for row in csvFile:\n",
    "        labels.append(row[:-1])\n",
    "labels = np.asarray(labels)\n",
    "main_labels = label_str_to_dec(labels[0:dataset_size], char2int)\n",
    "\n",
    "# Images\n",
    "image_prefix = \"FImg_ID_\"\n",
    "image_suffix = \".jpg\"\n",
    "images_str = [\n",
    "    \"{}{}{}{}\".format(path_final, image_prefix, img_idx, image_suffix)\n",
    "    for img_idx in range(1, dataset_size + 1)\n",
    "]\n",
    "main_dataset = pd.DataFrame({\"img_path\": images_str, \"label\": main_labels})\n",
    "\n",
    "# Split dataset\n",
    "generation_params = {\"dim\": image_dimension,\\\n",
    "                     \"nb_classes\": num_classes,\\\n",
    "                     \"column_img\": \"img_path\",\\\n",
    "                     \"column_label\": \"label\",}\n",
    "\n",
    "df_train, df_valid, df_test = split_dataset(data_frame=main_dataset,\\\n",
    "                                            rank=split_idx,\\\n",
    "                                            column_label=\"label\",\\\n",
    "                                            random_state=25)\n",
    "train_generator = DataGenerator(data_frame=df_train, batch_size=25, shuffle=True, **generation_params)\n",
    "valid_generator = DataGenerator(data_frame=df_valid, batch_size=100, shuffle=False, **generation_params)\n",
    "test_generator  = DataGenerator(data_frame=df_test, batch_size=100, shuffle=False, **generation_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build ensemble model and the new training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base models\n",
    "members = []\n",
    "for i in range(1, num_base_model+1):\n",
    "    modeli = load_model(path_bm + \"model_\" + str(i) + \".h5\")\n",
    "    members.append(modeli) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the stacked dataset\n",
    "X_train_stacked = stacked_dataset(members, train_generator)\n",
    "X_valid_stacked = stacked_dataset(members, valid_generator)\n",
    "X_test_stacked  = stacked_dataset(members, test_generator)\n",
    "\n",
    "y_train = aggregate_generator_labels(data_generator=train_generator)\n",
    "y_valid = aggregate_generator_labels(data_generator=valid_generator)\n",
    "y_test  = aggregate_generator_labels(data_generator=test_generator)\n",
    "\n",
    "y_test_dec = binary_label_to_decimal(y_test)\n",
    "print('Train size:',len(y_train),',',len(X_train_stacked))\n",
    "print('Validation size:',len(y_valid),',',len(X_valid_stacked))\n",
    "print('Test size:',len(y_test),',',len(X_test_stacked))\n",
    "print('Stacked train data size:', X_train_stacked.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ensemble model\n",
    "optimizer_name = 'Adam'\n",
    "lr = 0.0001\n",
    "optimizer = Adam(learning_rate=lr,beta_1=0.9,beta_2=0.999,epsilon=1e-07)\n",
    "model_stacked = build_stacked_model(num_layer,len(members),num_classes,optimizer)\n",
    "plot_model(model_stacked, to_file= path_out+'stacked_model.png')\n",
    "print(model_stacked.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the stacked model\n",
    "epochs = 500\n",
    "batch_size = 16\n",
    "\n",
    "# Early stopping\n",
    "monitor = 'val_loss'\n",
    "min_delta = 0.0001\n",
    "patience = 5\n",
    "early_stopping_loss = EarlyStopping(monitor=monitor,min_delta=min_delta,patience=patience,\\\n",
    "                                    verbose=0,mode='auto',baseline=None,restore_best_weights=True)\n",
    "model_stacked,history = fit_stacked_model(model_stacked, \\\n",
    "                                           X_train_stacked,\\\n",
    "                                           y_train,X_valid_stacked,\\\n",
    "                                           y_valid,\\\n",
    "                                           epochs,\\\n",
    "                                           batch_size,\\\n",
    "                                           early_stopping_loss)\n",
    "plot_EM_trian_history(history,path_out,IFPLOT=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained stacked model\n",
    "stacked_model_name = path_out+'stacked_model.h5'\n",
    "model_stacked.save(stacked_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write stacked model training hyperparameters\n",
    "hp_filename = path_out + \"stacked_model_hp_details.txt\"\n",
    "with open(hp_filename,\"w\") as f:\n",
    "    f.write('Early Stopping: %s (min_delta = %.8f, patience = %i) \\n' % (monitor, min_delta, patience))\n",
    "    f.write('Optimizer: %s (lr = %f) \\n' % (optimizer_name, lr))\n",
    "    f.write('Training epoch: %i \\n' % (epochs))\n",
    "    f.write('Batch size: %i \\n' % (batch_size))\n",
    "    f.write('Train size: %i \\n' % (len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions on test set\n",
    "y_pred_prob_stacked = model_stacked.predict(X_test_stacked) # predict\n",
    "y_pred_stacked = binary_label_to_decimal(prediction_standardized(y_pred_prob_stacked)) #compute ensemble model accuracy\n",
    "\n",
    "y_pred_prob = []\n",
    "y_pred = []\n",
    "for i in range(0,len(members)):\n",
    "    y_pred_prob_i = evaluate_model(members[i], evaluation_generator=test_generator, workers=6)\n",
    "    y_pred_i = binary_label_to_decimal(prediction_standardized(y_pred_prob_i))\n",
    "    y_pred_prob.append(y_pred_prob_i)\n",
    "    y_pred.append(y_pred_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute stacked model test accuracy\n",
    "y_test_dec = binary_label_to_decimal(y_test)\n",
    "stacked_acc_test = accuracy_score(y_test_dec,y_pred_stacked)\n",
    "acc_test = []\n",
    "for i in range(0,len(members)):\n",
    "    acc_test_i = accuracy_score(y_test_dec,y_pred[i])\n",
    "    acc_test.append(acc_test_i)\n",
    "    \n",
    "# Compare test accuracies\n",
    "mean = np.mean(acc_test)\n",
    "stdev = np.std(acc_test)\n",
    "print('TEST ACCURACY:')\n",
    "print('Ensemble model:',stacked_acc_test)\n",
    "print('Individual models: %f (%f)' % (mean, stdev))\n",
    "print('Individual accuracies:', acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test accuracies\n",
    "acc_filename = path_out + \"stacked_accuracy_summary.txt\"\n",
    "with open(acc_filename,\"w\") as f:\n",
    "    f.write('Ensemble: %.8f \\n' % (stacked_acc_test))\n",
    "    f.write('Individual models: %f (%f) \\n' % (mean, stdev))\n",
    "    f.write('Individual accuracies: {}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_multiclass(y, y_pred_prob):\n",
    "    return np.mean(np.sum((y-y_pred_prob)**2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top5(y_decimal,y_pred_prob):\n",
    "    correct = 0\n",
    "    for i in range(len(y_decimal)):\n",
    "        y_true = y_decimal[i]\n",
    "        y_pred = y_pred_prob[i]\n",
    "        top5_idx = sorted(range(len(y_pred)), key=lambda i: y_pred[i])[-5:]\n",
    "        if y_true in top5_idx:\n",
    "            correct = correct+1\n",
    "    top5_err = 1-correct/len(y_decimal)\n",
    "    return top5_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate on test set\n",
    "NLL =[]\n",
    "brier = []\n",
    "top_5 = []\n",
    "MSE = []\n",
    "for i in range(0,len(members)):\n",
    "    NLL_i = log_loss(y_test_dec,y_pred_prob[i]) \n",
    "    brier_i = brier_multiclass(y_test, y_pred_prob[i])\n",
    "    top_5_i = top5(y_test_dec,y_pred_prob[i])    \n",
    "    MSE_i = mean_squared_error(y_test,y_pred_prob[i]) # ensemble \n",
    "    \n",
    "    NLL.append(NLL_i)\n",
    "    brier.append(brier_i)\n",
    "    top_5.append(top_5_i)\n",
    "    MSE.append(MSE_i)\n",
    "\n",
    "NLL_mean = np.mean(NLL)\n",
    "brier_mean = np.mean(brier)\n",
    "MSE_mean = np.mean(MSE)\n",
    "top_5_mean = np.mean(top_5)\n",
    "print('NLL MEAN:',NLL_mean)\n",
    "print('NLL:',NLL)\n",
    "print('Brier MEAN:',brier_mean)\n",
    "print('Brier:',brier)\n",
    "print('MSE MEAN:',MSE_mean)\n",
    "print('MSE:',MSE)\n",
    "print('Top_5:',top_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save uncertainty in text file\n",
    "uncertainty_filename = path_out + \"ensemble_uncertainty_BM_\"+ str(len(members))+\".txt\"\n",
    "with open(uncertainty_filename,\"w\") as f:\n",
    "    f.write('NLL MEAN: %.8f \\n' % NLL_mean)\n",
    "    f.write('NLL: {}\\n'.format(NLL))\n",
    "    f.write('Brier MEAN: %.8f \\n'% brier_mean)\n",
    "    f.write('Brier: {}\\n'.format(brier))\n",
    "    f.write('MSE MEAN: %.8f \\n'% MSE_mean)\n",
    "    f.write('MSE: {}\\n'.format(MSE))\n",
    "    f.write('Top1 MEAN: %.8f \\n' % (mean))\n",
    "    f.write('Top1: {}\\n'.format(acc_test))\n",
    "    f.write('Top5 MEAN: %.8f \\n' % (top_5_mean))\n",
    "    f.write('Top5: {}\\n'.format(top_5))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate detailed performance improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [int(i)-1 for i in char_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "CM_stacked = confusion_matrix(y_test_dec, y_pred_stacked, labels=label_list)\n",
    "CM = []\n",
    "for i in range(0,len(members)):\n",
    "    CM_model_i = confusion_matrix(y_test_dec, y_pred[i], labels=label_list)\n",
    "    CM.append(CM_model_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hm(data,title,cbar=False ):\n",
    "    v = np.linspace(0, 100, 6, endpoint=True)\n",
    "    sns.heatmap(data,cmap=\"YlGnBu\",xticklabels=list(data.head()),yticklabels=list(data.index),\n",
    "                cbar=cbar,cbar_kws={'ticks':v},vmin=0, vmax=1)\n",
    "    plt.xlabel(' ',fontsize=30,fontname='Calibri',color='black')\n",
    "    plt.ylabel(' ',fontsize=30,fontname='Calibri',color='black')\n",
    "    plt.xticks(rotation='horizontal',fontsize=20,fontname='Calibri',color='black')\n",
    "    plt.yticks(rotation='horizontal',fontsize=20,fontname='Calibri',color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "df_cm_stacked = pd.DataFrame(CM_stacked,index = label_list,columns = label_list)\n",
    "df_cm = []\n",
    "for i in range(0,len(members)):\n",
    "    df_cm_i = pd.DataFrame(CM[i],index = label_list,columns = label_list)\n",
    "    df_cm.append(df_cm_i)\n",
    "\n",
    "v = np.linspace(0, 100, 6, endpoint=True)\n",
    "n_col = math.ceil((len(members)+1)/2)\n",
    "fig, axs = plt.subplots(2, 3,figsize=(15, 10))\n",
    "fig.suptitle('Confusion Matrix')\n",
    "for i in range(0,len(members)+1):\n",
    "    row = i//n_col\n",
    "    col = i%n_col\n",
    "    if i == 0:\n",
    "        sns.heatmap(ax=axs[row,col],data=df_cm_stacked, annot=True,cmap=\"YlGnBu\",\n",
    "                  cbar=False,cbar_kws={'ticks':v},vmin=0, vmax=100)\n",
    "        axs[row,col].set_title('Stacked model',fontsize=20,fontname='Calibri')\n",
    "    else:\n",
    "        sns.heatmap(ax=axs[row,col],data=df_cm[i-1], annot=True,cmap=\"YlGnBu\",\n",
    "                  cbar=False,cbar_kws={'ticks':v},vmin=0, vmax=100)\n",
    "        axs[row,col].set_title('Base model '+str(i),fontsize=20,fontname='Calibri')       \n",
    "CM_figname = path_out+'CM.png'\n",
    "plt.savefig(CM_figname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_by_class_stacked = CM_stacked.diagonal()/CM_stacked.sum(axis=1)\n",
    "acc_by_class = [acc_by_class_stacked]\n",
    "col_names = ['Stacked']\n",
    "for i in range(0,len(members)):\n",
    "    acc_by_class_model_i = CM[i].diagonal()/CM[i].sum(axis=1)\n",
    "    acc_by_class.append(acc_by_class_model_i)\n",
    "    col_names.append('Model'+str(i+1))\n",
    "\n",
    "acc_by_class_mean  = np.mean(acc_by_class,axis = 0)\n",
    "acc_by_class_stdev = np.std(acc_by_class,axis = 0)\n",
    "acc_by_class.append(acc_by_class_mean)\n",
    "acc_by_class.append(acc_by_class_stdev)\n",
    "col_names.append('MEAN')\n",
    "col_names.append('STDEV')\n",
    "\n",
    "print('ACCURACY BY CLASS')\n",
    "acc_by_class = pd.DataFrame(acc_by_class,index=col_names,columns=label_list)\n",
    "print(acc_by_class)\n",
    "acc_by_class.to_csv(path_out+'stacked_accuracy_by_class.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_stacked,tpr_stacked,roc_auc_stacked = compute_roc(label_list,y_test,y_pred_prob_stacked)\n",
    "fpr = []\n",
    "tpr = []\n",
    "roc_auc = []\n",
    "for i in range(len(members)):\n",
    "    fpr_i,tpr_i,roc_auc_i = compute_roc(label_list,y_test,y_pred_prob[i])\n",
    "    fpr.append(fpr_i)\n",
    "    tpr.append(tpr_i)\n",
    "    roc_auc.append(roc_auc_i)\n",
    "\n",
    "# Plot\n",
    "n_col = math.ceil((len(members)+1)/2)   \n",
    "fig, axs = plt.subplots(2, 3,figsize=(15, 10)) # change 3 to n_col for more BMs\n",
    "fig.suptitle('ROC')\n",
    "for i in range(len(members)+1):\n",
    "    row = i//n_col\n",
    "    col = i%n_col\n",
    "    if i == 0:\n",
    "        plot_roc(axs[row,col],label_list,fpr_stacked,tpr_stacked,roc_auc_stacked,'Stacked')\n",
    "    elif i == len(members):\n",
    "        plot_roc(axs[row,col],label_list,fpr[i-1],tpr[i-1],roc_auc[i-1],'Base Model '+str(i))\n",
    "    else:\n",
    "        plot_roc(axs[row,col],label_list,fpr[i-1],tpr[i-1],roc_auc[i-1],'Base Model '+str(i))\n",
    "plt.tight_layout()\n",
    "\n",
    "ROC_figname = path_out+'ROC.png'\n",
    "plt.savefig(ROC_figname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_tpr_stacked,std_tpr_stacked,ave_roc_auc_stacked = ave_roc(label_list,fpr_stacked,tpr_stacked,roc_auc_stacked)\n",
    "ave_tpr = []\n",
    "std_tpr = []\n",
    "ave_roc_auc = []\n",
    "for i in range(len(members)):\n",
    "    ave_tpr_i,std_tpr_i,ave_roc_auc_i = ave_roc(label_list,fpr[i],tpr[i],roc_auc[i])\n",
    "    ave_tpr.append(ave_tpr_i)\n",
    "    std_tpr.append(std_tpr_i)\n",
    "    ave_roc_auc.append(ave_roc_auc_i)\n",
    "\n",
    "# Plot\n",
    "n_col = math.ceil((len(members)+1)/2)   \n",
    "fig, axs = plt.subplots(2, 3,figsize=(15, 10))\n",
    "fig.suptitle('ROC')\n",
    "for i in range(len(members)+1):\n",
    "    row = i//n_col\n",
    "    col = i%n_col\n",
    "    if i == 0:\n",
    "        plot_ave_roc(axs[row,col],ave_tpr_stacked,std_tpr_stacked,ave_roc_auc_stacked,'Stacked')\n",
    "    else:\n",
    "        plot_ave_roc(axs[row,col],ave_tpr[i-1],std_tpr[i-1],ave_roc_auc[i-1],'Model '+str(i))\n",
    "plt.tight_layout()\n",
    "\n",
    "print('ROC AUC:', ave_roc_auc)\n",
    "\n",
    "AVE_ROC_figname = path_out+'AVE_ROC.png'\n",
    "plt.savefig(AVE_ROC_figname)\n",
    "\n",
    "ave_roc_filename = path_out + \"AVE_ROC.txt\"\n",
    "with open(ave_roc_filename,\"w\") as f:\n",
    "    f.write('Individual accuracies: {}'.format(ave_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "cmap = sns.color_palette(\"tab10\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "for i in range(len(members)+1):\n",
    "    if i==0:\n",
    "        ax.plot(fpr,ave_tpr_stacked,label='stacked,AUC = '+str(ave_roc_auc_stacked),linewidth=4,c=cmap[i])\n",
    "    else:\n",
    "        ax.plot(fpr,ave_tpr[i-1],label='Model '+str(i) + ',AUC = '+str(ave_roc_auc[i-1]),linewidth=4,ls = '--',c=cmap[i])\n",
    "\n",
    "plt.xticks(fontsize=20,fontname='Calibri', color=\"black\")\n",
    "plt.yticks(fontsize=20,fontname='Calibri', color=\"black\")\n",
    "plt.xlabel('FPR',fontsize=30,fontname='Calibri',color='black',y=-1)\n",
    "plt.ylabel('TPR',fontsize=30,fontname='Calibri',color='black')\n",
    "plt.legend(loc= 'lower center',bbox_to_anchor=(1.8, 0.5),fontsize=10,frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fpr = np.linspace(0, 1, 201)\n",
    "cmap = sns.color_palette(\"Blues\",10)\n",
    "rand_i = shuffle(np.linspace(3,8,6))\n",
    "cmap2 = sns.color_palette(\"Paired\")\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "for i in range(len(members)+1):\n",
    "    if i ==len(members):\n",
    "        ax.plot(fpr,ave_tpr_stacked,label='Stacked  ,AUC = '+str(ave_roc_auc_stacked),linewidth=6,c=cmap2[7])\n",
    "    else:\n",
    "        ax.plot(fpr,ave_tpr[i],label=str(i)+'             ,AUC = '+str(ave_roc_auc[i]),linewidth=6,c=cmap[int(rand_i[i])],linestyle='--')\n",
    "\n",
    "ax.set_xlim([0.0, 1.05])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate',fontsize=20,fontname='Calibri', color=\"black\")\n",
    "ax.set_ylabel('True Positive Rate',fontsize=20,fontname='Calibri', color=\"black\")\n",
    "ax.tick_params(axis='x', labelsize=20,color=\"black\",labelcolor=\"black\") \n",
    "ax.tick_params(axis='y', labelsize=20,color=\"black\",labelcolor=\"black\")  \n",
    "ax.set_xticks(np.round(np.linspace(0,1,6),2))\n",
    "ax.set_yticks(np.round(np.linspace(0,1,6),2))\n",
    "plt.legend(loc=\"right\",bbox_to_anchor=(2, 0.5),frameon=False)\n",
    "ave_roc_comparison_figname = path_out+'AVE_ROC_COMPARISON.png'\n",
    "plt.savefig(ave_roc_comparison_figname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
